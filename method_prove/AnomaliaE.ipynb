{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "import random as rd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import sys as s\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#           GERANDO O BANCO DE DADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dataset_Generator import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XC = (2, 2, 1)\n",
      "inp = torch.Size([2, 2, 300])\n",
      "out = torch.Size([2, 2, 300])\n"
     ]
    }
   ],
   "source": [
    "Dataset(2,2,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = pickle.load(open(\"inp\", \"rb\"))\n",
    "out = pickle.load(open(\"out\", \"rb\"))\n",
    "XC = pickle.load(open(\"XC\", \"rb\"))\n",
    "n_batch = np.shape(inp)[0]\n",
    "batch_size = np.shape(inp)[1]\n",
    "n_examples = np.shape(inp)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0.0000,   1.0033,   2.0067,   3.0100,   4.0134,   5.0167,   6.0201,\n",
      "          7.0234,   8.0268,   9.0301,  10.0334,  11.0368,  12.0401,  13.0435,\n",
      "         14.0468,  15.0502,  16.0535,  17.0569,  18.0602,  19.0635,  20.0669,\n",
      "         21.0702,  22.0736,  23.0769,  24.0803,  25.0836,  26.0870,  27.0903,\n",
      "         28.0936,  29.0970,  30.1003,  31.1037,  32.1070,  33.1104,  34.1137,\n",
      "         35.1171,  36.1204,  37.1237,  38.1271,  39.1304,  40.1338,  41.1371,\n",
      "         42.1405,  43.1438,  44.1472,  45.1505,  46.1538,  47.1572,  48.1605,\n",
      "         49.1639,  50.1672,  51.1706,  52.1739,  53.1773,  54.1806,  55.1839,\n",
      "         56.1873,  57.1906,  58.1940,  59.1973,  60.2007,  61.2040,  62.2074,\n",
      "         63.2107,  64.2140,  65.2174,  66.2207,  67.2241,  68.2274,  69.2308,\n",
      "         70.2341,  71.2375,  72.2408,  73.2441,  74.2475,  75.2508,  76.2542,\n",
      "         77.2575,  78.2609,  79.2642,  80.2676,  81.2709,  82.2742,  83.2776,\n",
      "         84.2809,  85.2843,  86.2876,  87.2910,  88.2943,  89.2977,  90.3010,\n",
      "         91.3043,  92.3077,  93.3110,  94.3144,  95.3177,  96.3211,  97.3244,\n",
      "         98.3278,  99.3311, 100.3344, 101.3378, 102.3411, 103.3445, 104.3478,\n",
      "        105.3512, 106.3545, 107.3579, 108.3612, 109.3645, 110.3679, 111.3712,\n",
      "        112.3746, 113.3779, 114.3813, 115.3846, 116.3880, 117.3913, 118.3946,\n",
      "        119.3980, 120.4013, 121.4047, 122.4080, 123.4114, 124.4147, 125.4181,\n",
      "        126.4214, 127.4247, 128.4281, 129.4314, 130.4348, 131.4381, 132.4415,\n",
      "        133.4448, 134.4482, 135.4515, 136.4548, 137.4582, 138.4615, 139.4649,\n",
      "        140.4682, 141.4716, 142.4749, 143.4783, 144.4816, 145.4849, 146.4883,\n",
      "        147.4916, 148.4950, 149.4983, 150.5017, 151.5050, 152.5084, 153.5117,\n",
      "        154.5151, 155.5184, 156.5217, 157.5251, 158.5284, 159.5318, 160.5351,\n",
      "        161.5385, 162.5418, 163.5452, 164.5485, 165.5518, 166.5552, 167.5585,\n",
      "        168.5619, 169.5652, 170.5686, 171.5719, 172.5753, 173.5786, 174.5819,\n",
      "        175.5853, 176.5886, 177.5920, 178.5953, 179.5987, 180.6020, 181.6054,\n",
      "        182.6087, 183.6120, 184.6154, 185.6187, 186.6221, 187.6254, 188.6288,\n",
      "        189.6321, 190.6355, 191.6388, 192.6421, 193.6455, 194.6488, 195.6522,\n",
      "        196.6555, 197.6589, 198.6622, 199.6656, 200.6689, 201.6722, 202.6756,\n",
      "        203.6789, 204.6823, 205.6856, 206.6890, 207.6923, 208.6957, 209.6990,\n",
      "        210.7023, 211.7057, 212.7090, 213.7124, 214.7157, 215.7191, 216.7224,\n",
      "        217.7258, 218.7291, 219.7324, 220.7358, 221.7391, 222.7425, 223.7458,\n",
      "        224.7492, 225.7525, 226.7559, 227.7592, 228.7625, 229.7659, 230.7692,\n",
      "        231.7726, 232.7759, 233.7793, 234.7826, 235.7860, 236.7893, 237.7926,\n",
      "        238.7960, 239.7993, 240.8027, 241.8060, 242.8094, 243.8127, 244.8161,\n",
      "        245.8194, 246.8227, 247.8261, 248.8294, 249.8328, 250.8361, 251.8395,\n",
      "        252.8428, 253.8462, 254.8495, 255.8528, 256.8562, 257.8595, 258.8629,\n",
      "        259.8662, 260.8696, 261.8729, 262.8763, 263.8796, 264.8829, 265.8863,\n",
      "        266.8896, 267.8930, 268.8963, 269.8997, 270.9030, 271.9064, 272.9097,\n",
      "        273.9130, 274.9164, 275.9197, 276.9231, 277.9264, 278.9298, 279.9331,\n",
      "        280.9365, 281.9398, 282.9431, 283.9465, 284.9498, 285.9532, 286.9565,\n",
      "        287.9599, 288.9632, 289.9666, 290.9699, 291.9732, 292.9766, 293.9799,\n",
      "        294.9833, 295.9866, 296.9900, 297.9933, 298.9967, 300.0000],\n",
      "       dtype=torch.float64)\n",
      "tensor([ 0.0000e+00,  8.4327e-01,  9.0649e-01,  1.3118e-01, -7.6548e-01,\n",
      "        -9.5405e-01, -2.6009e-01,  6.7445e-01,  9.8511e-01,  3.8451e-01,\n",
      "        -5.7177e-01, -9.9915e-01, -5.0228e-01,  4.5921e-01,  9.9592e-01,\n",
      "         6.1137e-01, -3.3871e-01, -9.7548e-01, -7.0990e-01,  2.1236e-01,\n",
      "         9.3818e-01,  7.9616e-01, -8.2337e-02, -8.8466e-01, -8.6865e-01,\n",
      "        -4.9109e-02,  8.1586e-01,  9.2613e-01,  1.7971e-01, -7.3296e-01,\n",
      "        -9.6761e-01, -3.0720e-01,  6.3738e-01,  9.9237e-01,  4.2938e-01,\n",
      "        -5.3079e-01, -9.9997e-01, -5.4414e-01,  4.1503e-01,  9.9029e-01,\n",
      "         6.4950e-01, -2.9210e-01, -9.6349e-01, -7.4363e-01,  1.6411e-01,\n",
      "         9.2005e-01,  8.2491e-01, -3.3295e-02, -8.6070e-01, -8.9193e-01,\n",
      "        -9.8100e-02,  7.8648e-01,  9.4354e-01,  2.2780e-01, -6.9866e-01,\n",
      "        -9.7884e-01, -3.5356e-01,  5.9877e-01,  9.9722e-01,  4.7321e-01,\n",
      "        -4.8853e-01, -9.9837e-01, -5.8469e-01,  3.6985e-01,  9.8227e-01,\n",
      "         6.8606e-01, -2.4478e-01, -9.4918e-01, -7.7557e-01,  1.1547e-01,\n",
      "         8.9970e-01,  8.5167e-01,  1.5828e-02, -8.3466e-01, -9.1306e-01,\n",
      "        -1.4685e-01,  7.5520e-01,  9.5867e-01,  2.7534e-01, -6.6268e-01,\n",
      "        -9.8771e-01, -3.9907e-01,  5.5872e-01,  9.9968e-01,  5.1591e-01,\n",
      "        -4.4509e-01, -9.9437e-01, -6.2382e-01,  3.2378e-01,  9.7187e-01,\n",
      "         7.2096e-01, -1.9687e-01, -9.3258e-01, -8.0563e-01,  6.6552e-02,\n",
      "         8.7717e-01,  8.7638e-01,  6.4912e-02, -8.0661e-01, -9.3199e-01,\n",
      "        -1.9525e-01,  7.2210e-01,  9.7149e-01,  3.2222e-01, -6.2511e-01,\n",
      "        -9.9419e-01, -4.4362e-01,  5.1731e-01,  9.9972e-01,  5.5735e-01,\n",
      "        -4.0058e-01, -9.8796e-01, -6.6145e-01,  2.7692e-01,  9.5914e-01,\n",
      "         7.5412e-01, -1.4848e-01, -9.1373e-01, -8.3375e-01,  1.7472e-02,\n",
      "         8.5254e-01,  8.9898e-01,  1.1384e-01, -7.7660e-01, -9.4867e-01,\n",
      "        -2.4318e-01,  6.8725e-01,  9.8196e-01,  3.6832e-01, -5.8602e-01,\n",
      "        -9.9828e-01, -4.8710e-01,  4.7466e-01,  9.9735e-01,  5.9745e-01,\n",
      "        -3.5510e-01, -9.7918e-01, -6.9749e-01,  2.2940e-01,  9.4408e-01,\n",
      "         7.8546e-01, -9.9736e-02, -8.9268e-01, -8.5986e-01, -3.1651e-02,\n",
      "         8.2584e-01,  9.1940e-01,  1.6249e-01, -7.4473e-01, -9.6305e-01,\n",
      "        -2.9052e-01,  6.5075e-01,  9.9006e-01,  4.1354e-01, -5.4552e-01,\n",
      "        -9.9995e-01, -5.2940e-01,  4.3086e-01,  9.9257e-01,  6.3612e-01,\n",
      "        -3.0876e-01, -9.6803e-01, -7.3184e-01,  1.8132e-01,  9.2675e-01,\n",
      "         8.1491e-01, -5.0751e-02, -8.6946e-01, -8.8390e-01, -8.0698e-02,\n",
      "         7.9715e-01,  9.3761e-01,  2.1075e-01, -7.1106e-01, -9.7512e-01,\n",
      "        -3.3717e-01,  6.1267e-01,  9.9577e-01,  4.5775e-01, -5.0370e-01,\n",
      "        -9.9922e-01, -5.7042e-01,  3.8603e-01,  9.8539e-01,  6.7324e-01,\n",
      "        -2.6168e-01, -9.5454e-01, -7.6442e-01,  1.3281e-01,  9.0719e-01,\n",
      "         8.4239e-01, -1.6441e-03, -8.4416e-01, -9.0580e-01, -1.2955e-01,\n",
      "         7.6654e-01,  9.5355e-01,  2.5851e-01, -6.7567e-01, -9.8483e-01,\n",
      "        -3.8299e-01,  5.7312e-01,  9.9908e-01,  5.0086e-01, -4.6067e-01,\n",
      "        -9.9607e-01, -6.1007e-01,  3.4026e-01,  9.7584e-01,  7.0874e-01,\n",
      "        -2.1397e-01, -9.3875e-01, -7.9516e-01,  8.3975e-02,  8.8543e-01,\n",
      "         8.6784e-01,  4.7467e-02, -8.1681e-01, -9.2551e-01, -1.7809e-01,\n",
      "         7.3407e-01,  9.6720e-01,  3.0563e-01, -6.3865e-01, -9.9216e-01,\n",
      "        -4.2790e-01,  5.3219e-01,  9.9998e-01,  5.4276e-01, -4.1653e-01,\n",
      "        -9.9052e-01, -6.4825e-01,  2.9367e-01,  9.6393e-01,  7.4253e-01,\n",
      "        -1.6574e-01, -9.2069e-01, -8.2398e-01,  3.4938e-02,  8.6154e-01,\n",
      "         8.9119e-01,  9.6464e-02, -7.8749e-01, -9.4300e-01, -2.2620e-01,\n",
      "         6.9984e-01,  9.7850e-01,  5.1209e-05, -8.4228e-05, -1.3503e-04,\n",
      "        -6.1645e-05,  6.1773e-05,  1.2146e-04,  6.8469e-05, -4.2058e-05,\n",
      "        -1.0736e-04, -7.2203e-05,  2.5062e-05,  9.3212e-05,  7.3348e-05,\n",
      "        -1.0700e-05, -7.9382e-05, -7.2373e-05, -1.1641e-06,  6.6169e-05,\n",
      "         6.9714e-05,  1.0707e-05, -5.3796e-05, -6.5764e-05, -1.8129e-05,\n",
      "         4.2420e-05,  6.0875e-05,  2.3648e-05, -3.2144e-05, -5.5356e-05,\n",
      "        -2.7490e-05,  2.3022e-05,  4.9470e-05,  2.9878e-05, -1.5065e-05,\n",
      "        -4.3440e-05, -3.1031e-05,  8.2556e-06,  3.7451e-05,  3.1159e-05,\n",
      "        -2.5478e-06, -3.1650e-05, -3.0455e-05, -2.1234e-06,  2.6150e-05,\n",
      "         2.9100e-05,  5.8374e-06, -2.1036e-05, -2.7253e-05, -8.6829e-06,\n",
      "         1.6365e-05,  2.5057e-05,  1.0754e-05, -1.2173e-05, -2.2638e-05])\n"
     ]
    }
   ],
   "source": [
    "print(inp[0][0])\n",
    "print(out[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "# DEFINE O MODELO\n",
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        # N, 50\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(1, 300),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(300, 600),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(600, 300),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(300, 1),\n",
    "            nn.ELU(),\n",
    "        )        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        return encoded\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------------\n",
    "# ------------------CHAMA O MODELO E INICIA CAMADAS DE PESOS ORTOGONAIS----------\n",
    "# -------------------------------------------------------------------------------\n",
    "model = Autoencoder()\n",
    "for m in model.modules():\n",
    "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.orthogonal_(m.weight)\n",
    "criterion = nn.MSELoss()  # segundo a investigar\n",
    "# ,lr=1e-4,weight_decay = 1e-5)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "# optimizer = torch.optim.SGD(model.parameters(),lr=1e-4,weight_decay = 1e-5)#,momentum=0.5)\n",
    "# -------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ------TREINO DO DECODER MODIFICADO--O[5]+Q[11] >> OUTPUT[1]--------------------\n",
    "# -------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x300 and 1x300)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/v/Desktop/Biology/method_prove/AnomaliaE.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/v/Desktop/Biology/method_prove/AnomaliaE.ipynb#X12sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m                 optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/v/Desktop/Biology/method_prove/AnomaliaE.ipynb#X12sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch:\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m,Loss:\u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/v/Desktop/Biology/method_prove/AnomaliaE.ipynb#X12sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m treine(\u001b[39m200\u001b[39;49m)\n",
      "\u001b[1;32m/home/v/Desktop/Biology/method_prove/AnomaliaE.ipynb Cell 10\u001b[0m in \u001b[0;36mtreine\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/v/Desktop/Biology/method_prove/AnomaliaE.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m A \u001b[39m=\u001b[39m A\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/v/Desktop/Biology/method_prove/AnomaliaE.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m O:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/v/Desktop/Biology/method_prove/AnomaliaE.ipynb#X12sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     recon, latent \u001b[39m=\u001b[39m model(i)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/v/Desktop/Biology/method_prove/AnomaliaE.ipynb#X12sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean((recon\u001b[39m-\u001b[39mA)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/v/Desktop/Biology/method_prove/AnomaliaE.ipynb#X12sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/v/Desktop/Biology/method_prove/AnomaliaE.ipynb Cell 10\u001b[0m in \u001b[0;36mAutoencoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/v/Desktop/Biology/method_prove/AnomaliaE.ipynb#X12sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/v/Desktop/Biology/method_prove/AnomaliaE.ipynb#X12sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     encoded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/v/Desktop/Biology/method_prove/AnomaliaE.ipynb#X12sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m encoded\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x300 and 1x300)"
     ]
    }
   ],
   "source": [
    "def treine(epochs):\n",
    "    inp = pickle.load(open(\"inp\", \"rb\"))\n",
    "    out = pickle.load(open(\"out\", \"rb\"))\n",
    "    n_batch = np.shape(inp)[0]\n",
    "    batch_size = np.shape(inp)[1]\n",
    "    n_examples = np.shape(inp)[2]\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx in range(n_batch):\n",
    "            O = inp[batch_idx]\n",
    "            A = out[batch_idx]\n",
    "            O = O.float()\n",
    "            A = A.float()\n",
    "            for i in O:\n",
    "                recon, latent = model(i)\n",
    "                loss = torch.mean((recon-A)**2)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        print(f'Epoch:{epoch+1},Loss:{loss.item():.4f}')\n",
    "\n",
    "\n",
    "\n",
    "treine(200)\n",
    "# print('end')\n",
    "# -------------------------------------------------------------------------------\n",
    "# --------------------- SALVANDO-------------------------------------------------\n",
    "# -------------------------------------------------------------------------------\n",
    "# PATH_save='Estado_Box_2_with_two_latent.pt'\n",
    "##PATH_load = 'Estado_Box_2_with_two_latent.pt'\n",
    "#torch.save(model.state_dict(), PATH_save)\n",
    "# s.exit()\n",
    "##model.load_state_dict(torch.load(PATH_load))\n",
    "#torch.save(model.state_dict(), PATH_save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "# GR√ÅFICOS\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Latent_values_Scynet():\n",
    "    for aux in range(n_batch):\n",
    "        O = inp[aux].float()\n",
    "        Q = question[aux].float()\n",
    "        A = out[aux].float()\n",
    "        j = J[aux]\n",
    "        x = np.zeros(np.shape(j)[0])\n",
    "        y1 = np.zeros(np.shape(j)[0])\n",
    "        y2 = np.zeros(np.shape(j)[0])\n",
    "        recon, latent = model(O, Q)\n",
    "        for i in range(0, 499):\n",
    "            x[i] = j[i]\n",
    "            y1[i] = latent[i, 0]\n",
    "            y2[i] = latent[i, 1]\n",
    "        plt.scatter(x, y1, label='Latent Activation 1')\n",
    "        plt.scatter(x, y2, label='Latent Activation 2')\n",
    "        plt.xlabel('Momento angular total')\n",
    "        plt.ylabel('Latent Activation')\n",
    "        plt.legend()\n",
    "        plt.pause(1.5)\n",
    "        plt.close()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "Latent_values_Scynet()\n",
    "# s.exit()\n",
    "# -------------------------------------------------------------------------------\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
